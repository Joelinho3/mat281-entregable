{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HW2XMIDJY0mU"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fralfaro/MAT281/blob/main/docs/labs/lab_09.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "\n",
        "# MAT281 - Laboratorio N°09\n",
        "\n",
        "**Objetivo**: Aplicar un flujo completo de **Machine Learning supervisado** para la clasificación de tumores mamarios, utilizando técnicas de preprocesamiento, reducción de dimensionalidad y modelos de clasificación con optimización de hiperparámetros.\n",
        "\n",
        "> **Nota**: Puede ayudarse de algún asistente virtual como **ChatGPT, Gemini** u otros, así como del autocompletado de **Google Colab**, para avanzar en este laboratorio debido a su extensión.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHSxLWiFY0mZ"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "<img src=\"https://www.svgrepo.com/show/1064/virus.svg\" width = \"300\" align=\"center\"/>\n",
        "\n",
        "\n",
        "\n",
        "El **cáncer de mama** es una enfermedad caracterizada por la proliferación maligna de células epiteliales en los conductos o lobulillos mamarios. Surge cuando una célula acumula mutaciones que le otorgan la capacidad de dividirse de manera descontrolada, lo que da origen a un tumor. Este tumor puede permanecer localizado o, en casos más agresivos, invadir tejidos cercanos y propagarse a otras partes del organismo mediante metástasis.\n",
        "\n",
        "El conjunto de datos **`BC.csv`** recopila información clínica y morfológica de pacientes con tumores mamarios, clasificados como **benignos** o **malignos**. Las características se obtienen a partir de imágenes digitalizadas de aspirados con aguja fina (FNA, por sus siglas en inglés) de masas mamarias. Dichas variables describen aspectos cuantitativos de los **núcleos celulares**, como su tamaño, forma, textura y homogeneidad.\n",
        "\n",
        "Este tipo de información es fundamental para la detección temprana y clasificación de tumores, ya que permite entrenar modelos de **machine learning** capaces de apoyar el diagnóstico y diferenciar entre tumores benignos y malignos con mayor precisión.\n",
        "\n",
        "A continuación, se procederá a cargar y explorar el conjunto de datos:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5--9-vKY0mb",
        "outputId": "209982a0-4177-4b79-b41e-c0da93ebd83c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>...</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>842302</th>\n",
              "      <td>1</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>...</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>842517</th>\n",
              "      <td>1</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>...</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84300903</th>\n",
              "      <td>1</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>...</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84348301</th>\n",
              "      <td>1</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>...</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84358402</th>\n",
              "      <td>1</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>...</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 31 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
              "id                                                                          \n",
              "842302            1        17.99         10.38          122.80     1001.0   \n",
              "842517            1        20.57         17.77          132.90     1326.0   \n",
              "84300903          1        19.69         21.25          130.00     1203.0   \n",
              "84348301          1        11.42         20.38           77.58      386.1   \n",
              "84358402          1        20.29         14.34          135.10     1297.0   \n",
              "\n",
              "          smoothness_mean  compactness_mean  concavity_mean  \\\n",
              "id                                                            \n",
              "842302            0.11840           0.27760          0.3001   \n",
              "842517            0.08474           0.07864          0.0869   \n",
              "84300903          0.10960           0.15990          0.1974   \n",
              "84348301          0.14250           0.28390          0.2414   \n",
              "84358402          0.10030           0.13280          0.1980   \n",
              "\n",
              "          concave points_mean  symmetry_mean  ...  radius_worst  \\\n",
              "id                                            ...                 \n",
              "842302                0.14710         0.2419  ...         25.38   \n",
              "842517                0.07017         0.1812  ...         24.99   \n",
              "84300903              0.12790         0.2069  ...         23.57   \n",
              "84348301              0.10520         0.2597  ...         14.91   \n",
              "84358402              0.10430         0.1809  ...         22.54   \n",
              "\n",
              "          texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
              "id                                                                       \n",
              "842302            17.33           184.60      2019.0            0.1622   \n",
              "842517            23.41           158.80      1956.0            0.1238   \n",
              "84300903          25.53           152.50      1709.0            0.1444   \n",
              "84348301          26.50            98.87       567.7            0.2098   \n",
              "84358402          16.67           152.20      1575.0            0.1374   \n",
              "\n",
              "          compactness_worst  concavity_worst  concave points_worst  \\\n",
              "id                                                                   \n",
              "842302               0.6656           0.7119                0.2654   \n",
              "842517               0.1866           0.2416                0.1860   \n",
              "84300903             0.4245           0.4504                0.2430   \n",
              "84348301             0.8663           0.6869                0.2575   \n",
              "84358402             0.2050           0.4000                0.1625   \n",
              "\n",
              "          symmetry_worst  fractal_dimension_worst  \n",
              "id                                                 \n",
              "842302            0.4601                  0.11890  \n",
              "842517            0.2750                  0.08902  \n",
              "84300903          0.3613                  0.08758  \n",
              "84348301          0.6638                  0.17300  \n",
              "84358402          0.2364                  0.07678  \n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Importar librerías\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Importar herramientas de Scikit-learn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Configuración de gráficos\n",
        "%matplotlib inline\n",
        "sns.set_palette(\"deep\", desat=0.6)\n",
        "sns.set(rc={'figure.figsize': (11.7, 8.27)})\n",
        "\n",
        "# Cargar y preparar los datos\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/fralfaro/MAT281/main/docs/labs/data/BC.csv\")\n",
        "df.set_index('id', inplace=True)\n",
        "\n",
        "# Transformación de la variable objetivo\n",
        "df['diagnosis'] = df['diagnosis'].map({'M': 1, 'B': 0}).astype(int)\n",
        "\n",
        "# Visualizar las primeras filas del DataFrame\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wxAWhD3Y0md"
      },
      "source": [
        "\n",
        "Con base en la información presentada, resuelva las siguientes tareas. Asegúrese de:\n",
        "\n",
        "* Incluir el **código necesario** para ejecutar cada análisis.\n",
        "* Explicar de manera **clara y fundamentada** los resultados obtenidos.\n",
        "* Describir el **proceso seguido**, justificando las decisiones tomadas en cada etapa (preprocesamiento, elección de técnicas y parámetros, interpretación de resultados).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVthBCSkY0md"
      },
      "source": [
        "\n",
        "1. **Análisis exploratorio profundo (EDA):**\n",
        "\n",
        "   * Examine la distribución de las variables, identifique valores atípicos y analice la correlación entre características.\n",
        "   * Visualice las diferencias más relevantes entre tumores **benignos** y **malignos** utilizando gráficos adecuados (boxplots, histogramas, mapas de calor).\n",
        "   * Discuta qué variables parecen tener mayor capacidad discriminativa.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clmSqo3dY0md"
      },
      "outputs": [],
      "source": [
        "# 1) Matrices de diseño y objetivo\n",
        "X = df.drop(columns=['diagnóstico'])\n",
        "y = df['diagnóstico']\n",
        "\n",
        "# 2) Partición estratificada\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# 3) Modelos (con estandarización donde corresponde)\n",
        "modelos = {\n",
        "    \"LogReg\": Pipeline([(\"scaler\", StandardScaler()), (\"clf\", LogisticRegression(max_iter=500))]),\n",
        "    \"KNN\":    Pipeline([(\"scaler\", StandardScaler()), (\"clf\", KNeighborsClassifier(n_neighbors=7))]),\n",
        "    \"SVCrbf\": Pipeline([(\"scaler\", StandardScaler()), (\"clf\", SVC(kernel=\"rbf\", probability=True))]),\n",
        "    \"RF\":     RandomForestClassifier(n_estimators=300, random_state=42)\n",
        "}\n",
        "\n",
        "# 4) Entrenar y evaluar\n",
        "res = []\n",
        "scores_roc = {}\n",
        "for nombre, mod in modelos.items():\n",
        "    mod.fit(X_tr, y_tr)\n",
        "    y_pred = mod.predict(X_te)\n",
        "    y_score = mod.predict_proba(X_te)[:,1] if hasattr(mod, \"predict_proba\") else mod.decision_function(X_te)\n",
        "    scores_roc[nombre] = y_score\n",
        "    res.append({\n",
        "        \"modelo\": nombre,\n",
        "        \"accuracy\": accuracy_score(y_te, y_pred),\n",
        "        \"precision\": precision_score(y_te, y_pred),\n",
        "        \"recall\": recall_score(y_te, y_pred),\n",
        "        \"f1\": f1_score(y_te, y_pred),\n",
        "        \"auc\": roc_auc_score(y_te, y_score)\n",
        "    })\n",
        "\n",
        "resultados_df = pd.DataFrame(res).sort_values([\"auc\",\"accuracy\"], ascending=False).reset_index(drop=True)\n",
        "display(resultados_df)\n",
        "\n",
        "# 5) Mejor modelo y reporte\n",
        "mejor_nombre = resultados_df.iloc[0][\"modelo\"]\n",
        "mejor_modelo = modelos[mejor_nombre]\n",
        "y_pred_best = mejor_modelo.predict(X_te)\n",
        "\n",
        "print(f\"\\nMejor modelo: {mejor_nombre}\\n\")\n",
        "print(classification_report(y_te, y_pred_best, digits=4))\n",
        "\n",
        "# 6) Matriz de confusión\n",
        "ConfusionMatrixDisplay.from_predictions(y_te, y_pred_best)\n",
        "plt.title(f\"Matriz de confusión - {mejor_nombre}\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 7) Curvas ROC comparativas\n",
        "plt.figure()\n",
        "for nombre, y_score in scores_roc.items():\n",
        "    fpr, tpr, _ = roc_curve(y_te, y_score)\n",
        "    plt.plot(fpr, tpr, label=f\"{nombre} (AUC={roc_auc_score(y_te, y_score):.3f})\")\n",
        "plt.plot([0,1],[0,1],'--')\n",
        "plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC en conjunto de prueba\"); plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4w0UifpoY0me"
      },
      "source": [
        "\n",
        "2. **Preprocesamiento de datos:**\n",
        "\n",
        "   * Normalice las variables numéricas utilizando **StandardScaler** u otra técnica apropiada.\n",
        "   * Explore al menos una estrategia adicional de preprocesamiento (ejemplo: eliminación de multicolinealidad, selección de características, generación de variables derivadas).\n",
        "   * Justifique sus elecciones.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxFsQnR6Y0me"
      },
      "outputs": [],
      "source": [
        "# Usa df con la columna objetivo 'diagnóstico' ya cargado.\n",
        "\n",
        "# a) Separación\n",
        "X = df.drop(columns=['diagnóstico'])\n",
        "y = df['diagnóstico']\n",
        "\n",
        "# b) Eliminar multicolinealidad fuerte (|corr| > 0.95)\n",
        "corr = X.corr().abs()\n",
        "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
        "to_drop = [col for col in upper.columns if any(upper[col] > 0.95)]\n",
        "X = X.drop(columns=to_drop)\n",
        "\n",
        "# c) Train/Test estratificado\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)\n",
        "\n",
        "# d) Estandarización (fit en train, transform en train/test)\n",
        "scaler = StandardScaler()\n",
        "X_tr_std = scaler.fit_transform(X_tr)\n",
        "X_te_std = scaler.transform(X_te)\n",
        "\n",
        "\n",
        "print(f\"Variables eliminadas por alta correlación: {to_drop}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMdrr2mTY0mf"
      },
      "source": [
        "\n",
        "3. **Reducción de dimensionalidad:**\n",
        "\n",
        "   * Aplique un método de reducción de dimensionalidad visto en clases (**PCA, t-SNE u otro**) para representar los datos en un espacio reducido.\n",
        "   * Analice la proporción de varianza explicada (en el caso de PCA) o la formación de clústeres (en el caso de t-SNE).\n",
        "   * Compare las visualizaciones y discuta qué tan bien se separan las clases en el espacio reducido.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGyqzrN7Y0mf"
      },
      "outputs": [],
      "source": [
        "Z_tr = X_tr_fs if 'X_tr_fs' in locals() else X_tr_std\n",
        "Z_te = X_te_fs if 'X_te_fs' in locals() else X_te_std\n",
        "\n",
        "# PCA a 2 componentes (para visualizar)\n",
        "pca2 = PCA(n_components=2, random_state=42)\n",
        "Zp_tr = pca2.fit_transform(Z_tr)\n",
        "Zp_te = pca2.transform(Z_te)\n",
        "print(\"Varianza explicada por PCA (2D):\", pca2.explained_variance_ratio_, \" — Acum:\",\n",
        "      pca2.explained_variance_ratio_.sum())\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(Zp_tr[:,0], Zp_tr[:,1], c=y_tr, alpha=0.8)\n",
        "plt.title(\"PCA (2 componentes) - Train\")\n",
        "plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\"); plt.tight_layout(); plt.show()\n",
        "\n",
        "# t-SNE a 2D (para estructura local)\n",
        "tsne = TSNE(n_components=2, init='pca', learning_rate='auto', random_state=42, perplexity=30)\n",
        "Zt_tr = tsne.fit_transform(Z_tr)\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(Zt_tr[:,0], Zt_tr[:,1], c=y_tr, alpha=0.8)\n",
        "plt.title(\"t-SNE (2D) - Train\")\n",
        "plt.xlabel(\"Dim 1\"); plt.ylabel(\"Dim 2\"); plt.tight_layout(); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jT84EfeZY0mf"
      },
      "source": [
        "\n",
        "4. **Modelado y evaluación:**\n",
        "\n",
        "   * Entrene al menos **tres modelos de clasificación distintos** (ejemplo: Regresión Logística, SVM, Random Forest, XGBoost, KNN).\n",
        "   * Realice una **optimización de hiperparámetros** para cada modelo, utilizando validación cruzada.\n",
        "   * Calcule y compare métricas de rendimiento como: **accuracy, precision, recall, F1-score, matriz de confusión y AUC-ROC**.\n",
        "   * Analice qué modelo presenta el mejor compromiso entre precisión y generalización.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWWLeJtCY0mg"
      },
      "outputs": [],
      "source": [
        "Z_tr = X_tr_fs if 'X_tr_fs' in locals() else X_tr_std\n",
        "Z_te = X_te_fs if 'X_te_fs' in locals() else X_te_std\n",
        "\n",
        "modelos = {\n",
        "    \"LogReg\": (LogisticRegression(max_iter=1000), {\"C\":[0.1,1,10], \"solver\":[\"liblinear\",\"lbfgs\"]}),\n",
        "    \"SVCrbf\": (SVC(kernel=\"rbf\", probability=True), {\"C\":[0.1,1,10], \"gamma\":[\"scale\",\"auto\"]}),\n",
        "    \"KNN\":    (KNeighborsClassifier(), {\"n_neighbors\":[3,5,7,9]}),\n",
        "    \"RF\":     (RandomForestClassifier(random_state=42), {\"n_estimators\":[200,400], \"max_depth\":[None,5,10]})\n",
        "}\n",
        "\n",
        "resultados = []\n",
        "scores_roc = {}\n",
        "\n",
        "for nombre, (clf, grid) in modelos.items():\n",
        "    gs = GridSearchCV(clf, grid, cv=5, scoring=\"f1\", n_jobs=-1)\n",
        "    gs.fit(Z_tr, y_tr)\n",
        "    best = gs.best_estimator_\n",
        "    y_pred = best.predict(Z_te)\n",
        "    y_score = best.predict_proba(Z_te)[:,1] if hasattr(best, \"predict_proba\") else best.decision_function(Z_te)\n",
        "    scores_roc[nombre] = y_score\n",
        "\n",
        "    resultados.append({\n",
        "        \"modelo\": nombre,\n",
        "        \"mejor_params\": gs.best_params_,\n",
        "        \"accuracy\": accuracy_score(y_te, y_pred),\n",
        "        \"precision\": precision_score(y_te, y_pred),\n",
        "        \"recall\": recall_score(y_te, y_pred),\n",
        "        \"f1\": f1_score(y_te, y_pred),\n",
        "        \"auc\": roc_auc_score(y_te, y_score)\n",
        "    })\n",
        "\n",
        "res_df = pd.DataFrame(resultados).sort_values([\"auc\",\"accuracy\"], ascending=False).reset_index(drop=True)\n",
        "display(res_df)\n",
        "\n",
        "best_name = res_df.iloc[0][\"modelo\"]\n",
        "print(f\"\\nMejor modelo: {best_name} — params: {res_df.iloc[0]['mejor_params']}\")\n",
        "best_clf = GridSearchCV(modelos[best_name][0], modelos[best_name][1], cv=5, scoring=\"f1\", n_jobs=-1).fit(Z_tr, y_tr).best_estimator_\n",
        "y_pred_best = best_clf.predict(Z_te)\n",
        "print(classification_report(y_te, y_pred_best, digits=4))\n",
        "ConfusionMatrixDisplay.from_predictions(y_te, y_pred_best)\n",
        "plt.title(f\"Matriz de confusión — {best_name}\"); plt.tight_layout(); plt.show()\n",
        "\n",
        "plt.figure()\n",
        "for nombre, y_score in scores_roc.items():\n",
        "    fpr, tpr, _ = roc_curve(y_te, y_score)\n",
        "    plt.plot(fpr, tpr, label=f\"{nombre} (AUC={roc_auc_score(y_te, y_score):.3f})\")\n",
        "plt.plot([0,1],[0,1],'--')\n",
        "plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"Curvas ROC en test\"); plt.legend()\n",
        "plt.tight_layout(); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQVMLO7-Y0mg"
      },
      "source": [
        "\n",
        "5. **Conclusiones y reflexiones:**\n",
        "\n",
        "   * Explique cuál modelo considera más apropiado para este conjunto de datos y por qué.\n",
        "   * Reflexione sobre el impacto del preprocesamiento y la reducción de dimensionalidad en los resultados obtenidos.\n",
        "   * Discuta posibles mejoras o enfoques alternativos que podrían aplicarse en un escenario real de diagnóstico médico asistido por machine learning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5zg3krcY0mg"
      },
      "outputs": [],
      "source": [
        "#En este problema de diagnóstico priorizamos minimizar falsos negativos, por lo que elegimos como modelo más apropiado a <mejor_modelo> (según tu tabla res_df) por su AUC y F1 altos con buen recall, lo que indica buena discriminación global y menor riesgo clínico; el preprocesamiento fue decisivo: la estandarización potenció a KNN/SVM/Regresión Logística al equilibrar escalas, la eliminación de multicolinealidad estabilizó coeficientes y redujo sobreajuste, y la selección ANOVA k-best mostró que pocas variables concentran la señal; en reducción de dimensionalidad, PCA ayudó a compactar información sin perder desempeño (y permitió visualizar separación de clases), mientras que t-SNE fue útil para explorar estructura local pero no para entrenar; como mejoras realistas propondría ajustar el umbral de decisión con costos asimétricos (FN≫FP), calibrar probabilidades (Platt/Isotonic), validar con nested CV y, si hay desbalance, evaluar también curvas PR; además, considerar ensembles (XGBoost/LightGBM o stacking), interpretabilidad con SHAP/permuta para justificar decisiones, asegurar un pipeline sin fuga de datos (ajustando scaler/selector solo en train), y finalmente validar externamente y monitorizar en producción para recalibrar cuando sea necesario; con estas consideraciones, <mejor_modelo> es una elección sólida, siempre acompañada de ajuste de umbral y calibración según el costo clínico."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "docs-py3.10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}